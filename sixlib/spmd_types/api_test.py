"""
Tests for SPMD type system.

Uses PyTorch's LocalTensorMode to simulate distributed operations in a single
process without requiring an actual distributed backend.
"""

import unittest

import expecttest
import torch
import torch.distributed as dist
from sixlib.spmd_types import (
    all_gather,
    all_reduce,
    all_to_all,
    convert,
    get_mesh,
    I,
    Invariant,
    P,
    Partial,
    PartitionSpec,
    R,
    redistribute,
    reduce_scatter,
    reinterpret,
    Replicate,
    S,
    set_mesh,
    Shard,
    V,
    Varying,
)
from sixlib.spmd_types._checker import (
    _infer_mul_output_type_for_axis,
    assert_local_type,
    get_axis_local_type,
    infer_output_type_for_axis,
    SpmdTypeMode,
)
from sixlib.spmd_types._collectives import (
    _AllReduce,
)
from sixlib.spmd_types._local import (
    _ConvertInvariantToPartial,
    _ConvertInvariantToVarying,
    _ConvertReplicateToPartial,
    _ConvertReplicateToVarying,
    _ConvertVaryingToPartial,
    _InvariantToReplicate,
    _ReplicateToInvariant,
    _ReplicateToPartial,
    _ReplicateToVarying,
    _VaryingToPartial,
)
from sixlib.spmd_types.types import SpmdTypeError
from torch.distributed._local_tensor import LocalTensor, LocalTensorMode
from torch.testing._internal.distributed.fake_pg import FakeStore


class FakeMesh:
    """
    A fake DeviceMesh for testing that returns the default process group.

    In real distributed code, DeviceMesh maps axis names to process groups.
    For testing with LocalTensorMode, we just return the default group.
    """

    def __init__(self, world_size: int):
        self.world_size = world_size

    def get_group(self, axis_name: str):
        """Return the default process group for any axis name."""
        return dist.distributed_c10d._get_default_group()


class LocalTensorTestCase(unittest.TestCase):
    """
    Base test class that sets up LocalTensorMode and fake process group.
    """

    WORLD_SIZE = 3

    @classmethod
    def setUpClass(cls):
        """Initialize fake distributed environment."""
        if not dist.is_initialized():
            store = FakeStore()
            dist.init_process_group(
                backend="fake", rank=0, world_size=cls.WORLD_SIZE, store=store
            )
        # Set up the global mesh
        set_mesh(FakeMesh(cls.WORLD_SIZE))

    @classmethod
    def tearDownClass(cls):
        """Clean up distributed environment."""
        set_mesh(None)
        if dist.is_initialized():
            dist.destroy_process_group()

    def setUp(self):
        """Enter LocalTensorMode and SpmdTypeMode for each test."""
        self.mode = LocalTensorMode(self.WORLD_SIZE)
        self.mode.__enter__()
        self.spmd_mode = SpmdTypeMode()
        self.spmd_mode.__enter__()

    def tearDown(self):
        """Exit SpmdTypeMode and LocalTensorMode after each test."""
        self.spmd_mode.__exit__(None, None, None)
        self.mode.__exit__(None, None, None)

    def _generate_inputs(self, shape, axis, typ):
        """Generate input tensor with the given SPMD type on the given axis.

        Data generation:
          R, I -> same data on all ranks (replicated)
          V, P, S(i) -> different data per rank
        """
        if typ is R or typ is I:
            base = torch.randn(shape)
            result = self.mode.rank_map(lambda r: base.clone())
        else:
            result = self.mode.rank_map(lambda r: torch.randn(shape) + r)
        assert_local_type(result, {axis: typ})
        return result

    def _assert_all_ranks_equal(self, lt, msg=""):
        """Assert that a LocalTensor has the same value on all ranks."""
        tensors = lt._local_tensors
        ranks = list(tensors.keys())
        for i in range(1, len(ranks)):
            torch.testing.assert_close(
                tensors[ranks[0]],
                tensors[ranks[i]],
                msg=f"{msg}: rank 0 vs rank {ranks[i]}",
            )

    def _assert_ranks_different(self, lt, msg=""):
        """Assert that a LocalTensor has different values on different ranks."""
        tensors = lt._local_tensors
        ranks = list(tensors.keys())
        all_same = all(
            torch.allclose(tensors[ranks[0]], tensors[ranks[i]])
            for i in range(1, len(ranks))
        )
        self.assertFalse(all_same, f"{msg}: expected different values per rank")

    def _check_backward_not_none(
        self, autograd_fn_class, forward_args, backward_grad_out
    ):
        """
        Verify that an autograd Function's backward returns valid gradients (not None).

        This directly tests the backward method of autograd Functions to catch bugs
        like returning None instead of the actual gradient tensor.

        Args:
            autograd_fn_class: The torch.autograd.Function class to test
            forward_args: Tuple of args to pass to forward (will also set up ctx)
            backward_grad_out: The gradient output to pass to backward

        Returns:
            The result of the backward call
        """

        # Create a context object
        class FakeCtx:
            pass

        ctx = FakeCtx()

        # Run forward to populate ctx
        autograd_fn_class.forward(ctx, *forward_args)

        # Run backward
        result = autograd_fn_class.backward(ctx, backward_grad_out)

        # Check that the gradient for the main input (first element) is not None
        if isinstance(result, tuple):
            grad_input = result[0]
        else:
            grad_input = result

        self.assertIsNotNone(
            grad_input, f"{autograd_fn_class.__name__}.backward returned None"
        )

        # Also verify it's a proper tensor/LocalTensor with the right shape
        if isinstance(backward_grad_out, LocalTensor):
            self.assertIsInstance(grad_input, LocalTensor)
        else:
            self.assertIsInstance(grad_input, torch.Tensor)

        return result


class TestEinsumTypePropagation(unittest.TestCase):
    """Test einsum type propagation rules.

    TODO: These tests depend on LTensor which was removed. They need to be
    reimplemented once the type-checking layer is rebuilt.
    """

    pass


class TestAllReduce(LocalTensorTestCase):
    """Test all_reduce operation: P -> R | I."""

    def test_all_reduce_p_to_r(self):
        """all_reduce(R): P -> R, sums across ranks. Tests forward and backward."""
        # Create "partial" input - different values per rank that need summing
        x = self._generate_inputs((4,), "tp", P)

        # Get expected sum before all_reduce modifies x in-place
        expected_sum = sum(x._local_tensors[r].clone() for r in range(self.WORLD_SIZE))

        # Run all_reduce
        result = all_reduce(x, "tp", src=P, dst=R)

        # Check all ranks have the same summed value
        self._assert_all_ranks_equal(
            result, "all_reduce result should be same on all ranks"
        )
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], expected_sum)
        self.assertIs(get_axis_local_type(result, "tp"), R)

        # Backward check: verify backward returns valid gradient
        grad_out = self._generate_inputs((4,), "tp", P)
        self._check_backward_not_none(_AllReduce, (x, "tp", R, False), grad_out)

    def test_all_reduce_p_to_i(self):
        """all_reduce(I): P -> I, sums across ranks. Tests forward and backward."""
        x = self._generate_inputs((4,), "tp", P)
        # Get expected sum before all_reduce modifies x in-place
        expected_sum = sum(x._local_tensors[r].clone() for r in range(self.WORLD_SIZE))

        result = all_reduce(x, "tp", src=P, dst=I)

        self._assert_all_ranks_equal(
            result, "all_reduce result should be same on all ranks"
        )
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], expected_sum)
        self.assertIs(get_axis_local_type(result, "tp"), I)

        # Backward check: verify backward returns valid gradient
        grad_out = self._generate_inputs((4,), "tp", I)
        self._check_backward_not_none(_AllReduce, (x, "tp", I, False), grad_out)

    def test_all_reduce_invalid_src(self):
        """all_reduce only accepts partial src."""
        x = self._generate_inputs((4,), "tp", R)
        with self.assertRaises(ValueError) as ctx:
            all_reduce(x, "tp", src=R, dst=R)
        self.assertIn("must be P", str(ctx.exception))

    def test_all_reduce_invalid_dst(self):
        """all_reduce only accepts replicate or invariant dst."""
        x = self._generate_inputs((4,), "tp", P)
        with self.assertRaises(ValueError) as ctx:
            all_reduce(x, "tp", src=P, dst=V)
        self.assertIn("must be R or I", str(ctx.exception))

    def test_all_reduce_p_to_r_inplace(self):
        """all_reduce(R, inplace=True): P -> R in-place. Result shares storage with input."""
        x = self._generate_inputs((4,), "tp", P)
        expected_sum = sum(x._local_tensors[r].clone() for r in range(self.WORLD_SIZE))

        # Save data pointers before
        input_ptrs = {r: x._local_tensors[r].data_ptr() for r in range(self.WORLD_SIZE)}

        result = all_reduce(x, "tp", src=P, dst=R, inplace=True)

        # Check correctness
        self._assert_all_ranks_equal(
            result, "all_reduce inplace result should be same on all ranks"
        )
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], expected_sum)
        self.assertIs(get_axis_local_type(result, "tp"), R)

        # Check in-place: returned tensor shares storage with input
        for r in range(self.WORLD_SIZE):
            self.assertEqual(
                result._local_tensors[r].data_ptr(),
                input_ptrs[r],
                f"rank {r}: inplace result should share storage with input",
            )

    def test_all_reduce_p_to_i_inplace(self):
        """all_reduce(I, inplace=True): P -> I in-place. Result shares storage with input."""
        x = self._generate_inputs((4,), "tp", P)
        expected_sum = sum(x._local_tensors[r].clone() for r in range(self.WORLD_SIZE))

        # Save data pointers before
        input_ptrs = {r: x._local_tensors[r].data_ptr() for r in range(self.WORLD_SIZE)}

        result = all_reduce(x, "tp", src=P, dst=I, inplace=True)

        # Check correctness
        self._assert_all_ranks_equal(
            result, "all_reduce inplace result should be same on all ranks"
        )
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], expected_sum)
        self.assertIs(get_axis_local_type(result, "tp"), I)

        # Check in-place: returned tensor shares storage with input
        for r in range(self.WORLD_SIZE):
            self.assertEqual(
                result._local_tensors[r].data_ptr(),
                input_ptrs[r],
                f"rank {r}: inplace result should share storage with input",
            )

    def test_all_reduce_preserves_other_axes(self):
        """all_reduce preserves SPMD types on other mesh axes."""
        x = self.mode.rank_map(lambda r: torch.randn(4) + r)
        assert_local_type(x, {"tp": P, "dp": R})
        result = all_reduce(x, "tp", src=P, dst=R)
        self.assertIs(get_axis_local_type(result, "tp"), R)
        self.assertIs(get_axis_local_type(result, "dp"), R)

    def test_all_reduce_type_mismatch(self):
        """all_reduce raises SpmdTypeError when input type doesn't match src."""
        x = self._generate_inputs((4,), "tp", R)
        with self.assertRaises(SpmdTypeError):
            all_reduce(x, "tp", src=P, dst=R)


class TestAllGather(LocalTensorTestCase):
    """Test all_gather operation: V -> R | I."""

    def test_all_gather_v_to_r(self):
        """all_gather(R): V -> R, gathers shards from all ranks."""
        # Create varying input - different per rank
        x = self.mode.rank_map(lambda r: torch.tensor(float(r)))
        assert_local_type(x, {"tp": V})

        result = all_gather(x, "tp", src=V, dst=R)

        # Result should be [0, 1, 2] on all ranks (stack)
        expected = torch.tensor([0.0, 1.0, 2.0])
        self._assert_all_ranks_equal(result)
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], expected)
        self.assertIs(get_axis_local_type(result, "tp"), R)

    def test_all_gather_v_to_i(self):
        """all_gather(I): V -> I, gathers shards from all ranks."""
        x = self.mode.rank_map(lambda r: torch.tensor(float(r) * 2))
        assert_local_type(x, {"tp": V})

        result = all_gather(x, "tp", src=V, dst=I)

        expected = torch.tensor([0.0, 2.0, 4.0])
        self._assert_all_ranks_equal(result)
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], expected)
        self.assertIs(get_axis_local_type(result, "tp"), I)

    def test_all_gather_shard_to_r(self):
        """all_gather(R): S(i) -> R, gathers shards from all ranks."""
        # Create sharded input on dim 0
        x = self.mode.rank_map(lambda r: torch.tensor([float(r), float(r) + 0.5]))
        assert_local_type(x, {"tp": V})

        result = all_gather(x, "tp", src=S(0), dst=R)

        # Result should be concatenated shards
        expected = torch.tensor([0.0, 0.5, 1.0, 1.5, 2.0, 2.5])
        self._assert_all_ranks_equal(result)
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], expected)
        self.assertIs(get_axis_local_type(result, "tp"), R)

    def test_all_gather_invalid_src(self):
        """all_gather only accepts V or S(i) src."""
        x = self._generate_inputs((4,), "tp", R)
        with self.assertRaises(ValueError) as ctx:
            all_gather(x, "tp", src=R, dst=R)
        self.assertIn("must be V or S(i)", str(ctx.exception))

    def test_all_gather_invalid_dst(self):
        """all_gather only accepts replicate or invariant dst."""
        x = self._generate_inputs((4,), "tp", V)
        with self.assertRaises(ValueError) as ctx:
            all_gather(x, "tp", src=V, dst=P)
        self.assertIn("must be R or I", str(ctx.exception))


class TestReduceScatter(LocalTensorTestCase):
    """Test reduce_scatter operation: P -> V."""

    def test_reduce_scatter(self):
        """reduce_scatter: P -> V, reduces and scatters."""
        # Create input with world_size chunks per rank
        # Each rank has [A_r, B_r, C_r] where total length is world_size * chunk_size
        chunk_size = 2
        x = self.mode.rank_map(
            lambda r: torch.arange(
                self.WORLD_SIZE * chunk_size, dtype=torch.float
            ).reshape(self.WORLD_SIZE, chunk_size)
            + r
        )
        assert_local_type(x, {"tp": P})

        result = reduce_scatter(x, "tp", src=P, dst=V, scatter_dim=0)

        # Each rank r gets the sum of chunk r from all ranks
        for r in range(self.WORLD_SIZE):
            expected = torch.zeros(chunk_size)
            for src_rank in range(self.WORLD_SIZE):
                expected += x._local_tensors[src_rank][r]
            torch.testing.assert_close(
                result._local_tensors[r], expected, msg=f"rank {r}"
            )
        self.assertIs(get_axis_local_type(result, "tp"), V)

    def test_reduce_scatter_to_shard(self):
        """reduce_scatter: P -> S(i), reduces and scatters."""
        chunk_size = 2
        x = self.mode.rank_map(
            lambda r: torch.arange(self.WORLD_SIZE * chunk_size, dtype=torch.float) + r
        )
        assert_local_type(x, {"tp": P})

        result = reduce_scatter(x, "tp", src=P, dst=S(0), scatter_dim=1)

        for r in range(self.WORLD_SIZE):
            expected = torch.zeros(chunk_size)
            for src_rank in range(self.WORLD_SIZE):
                chunk_start = r * chunk_size
                chunk_end = (r + 1) * chunk_size
                expected += x._local_tensors[src_rank][chunk_start:chunk_end]
            torch.testing.assert_close(
                result._local_tensors[r], expected, msg=f"rank {r}"
            )
        self.assertIs(get_axis_local_type(result, "tp"), V)

    def test_reduce_scatter_invalid_src(self):
        """reduce_scatter only accepts P (or V, which is auto-reinterpreted) src."""
        x = self._generate_inputs((6,), "tp", R)
        with self.assertRaises(ValueError) as ctx:
            reduce_scatter(x, "tp", src=R, dst=V)
        self.assertIn("must be P", str(ctx.exception))

    def test_reduce_scatter_invalid_dst(self):
        """reduce_scatter only accepts V or S(i) dst."""
        x = self._generate_inputs((6,), "tp", P)
        with self.assertRaises(ValueError) as ctx:
            reduce_scatter(x, "tp", src=P, dst=R)
        self.assertIn("must be V or S(i)", str(ctx.exception))


class TestAllToAll(LocalTensorTestCase):
    """Test all_to_all operation: V -> V."""

    def test_all_to_all(self):
        """all_to_all: V -> V, transposes mesh and tensor dims."""
        # Create input: rank r has [r*3, r*3+1, r*3+2]
        # After all_to_all, rank r should get [r, r+3, r+6]
        x = self.mode.rank_map(
            lambda r: torch.tensor([float(r * 3 + i) for i in range(self.WORLD_SIZE)])
        )
        assert_local_type(x, {"tp": V})

        result = all_to_all(x, "tp", src=V, dst=V, split_dim=0, concat_dim=0)

        # Check result
        for r in range(self.WORLD_SIZE):
            expected = torch.tensor([float(r + i * 3) for i in range(self.WORLD_SIZE)])
            torch.testing.assert_close(
                result._local_tensors[r], expected, msg=f"rank {r}"
            )
        self.assertIs(get_axis_local_type(result, "tp"), V)

    def test_all_to_all_shard_to_shard(self):
        """all_to_all: S(i) -> S(j), resharding between dimensions."""
        # Each rank has a 2D tensor
        x = self.mode.rank_map(
            lambda r: torch.arange(6, dtype=torch.float).reshape(3, 2) + r * 10
        )
        assert_local_type(x, {"tp": V})

        result = all_to_all(x, "tp", src=S(0), dst=S(1), split_dim=1, concat_dim=0)

        # Check shapes are correct
        for r in range(self.WORLD_SIZE):
            self.assertEqual(result._local_tensors[r].shape[0], 1)
            self.assertEqual(result._local_tensors[r].shape[1], 6)
        self.assertIs(get_axis_local_type(result, "tp"), V)

    def test_all_to_all_invalid_src(self):
        """all_to_all only accepts V or S(i) src."""
        x = self._generate_inputs((6,), "tp", R)
        with self.assertRaises(ValueError) as ctx:
            all_to_all(x, "tp", src=R, dst=V)
        self.assertIn("must be V or S(i)", str(ctx.exception))

    def test_all_to_all_invalid_dst(self):
        """all_to_all only accepts V or S(i) dst."""
        x = self._generate_inputs((6,), "tp", V)
        with self.assertRaises(ValueError) as ctx:
            all_to_all(x, "tp", src=V, dst=R)
        self.assertIn("must be V or S(i)", str(ctx.exception))


class TestReinterpret(LocalTensorTestCase):
    """Test reinterpret operations (no-op forwards, possibly comms in backwards)."""

    def test_reinterpret_r_to_v(self):
        """reinterpret(R,V): R -> V, no-op forward. Tests forward and backward."""
        x = self._generate_inputs((4,), "tp", R)
        original = {r: x._local_tensors[r].clone() for r in range(self.WORLD_SIZE)}

        result = reinterpret(x, "tp", src=R, dst=V)

        # Forward is no-op, values unchanged
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], original[r])
        self.assertIs(get_axis_local_type(result, "tp"), V)

        # Backward check
        grad_out = self._generate_inputs((4,), "tp", V)
        self._check_backward_not_none(_ReplicateToVarying, (x, "tp"), grad_out)

    def test_reinterpret_r_to_i(self):
        """reinterpret(R,I): R -> I, no-op forward. Tests forward and backward."""
        x = self._generate_inputs((4,), "tp", R)
        original = {r: x._local_tensors[r].clone() for r in range(self.WORLD_SIZE)}

        result = reinterpret(x, "tp", src=R, dst=I)

        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], original[r])
        self.assertIs(get_axis_local_type(result, "tp"), I)

        # Backward check
        grad_out = self._generate_inputs((4,), "tp", I)
        self._check_backward_not_none(_ReplicateToInvariant, (x, "tp"), grad_out)

    def test_reinterpret_r_to_p(self):
        """reinterpret(R,P): R -> P, no-op forward. Tests forward and backward."""
        x = self._generate_inputs((4,), "tp", R)
        original = {r: x._local_tensors[r].clone() for r in range(self.WORLD_SIZE)}

        result = reinterpret(x, "tp", src=R, dst=P)

        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], original[r])
        self.assertIs(get_axis_local_type(result, "tp"), P)

        # Backward check
        grad_out = self._generate_inputs((4,), "tp", R)
        self._check_backward_not_none(_ReplicateToPartial, (x, "tp"), grad_out)

    def test_reinterpret_i_to_r(self):
        """reinterpret(I,R): I -> R, no-op forward. Tests forward and backward."""
        x = self._generate_inputs((4,), "tp", I)
        original = {r: x._local_tensors[r].clone() for r in range(self.WORLD_SIZE)}

        result = reinterpret(x, "tp", src=I, dst=R)

        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], original[r])
        self.assertIs(get_axis_local_type(result, "tp"), R)

        # Backward check
        grad_out = self._generate_inputs((4,), "tp", P)
        self._check_backward_not_none(_InvariantToReplicate, (x, "tp"), grad_out)

    def test_reinterpret_v_to_p(self):
        """reinterpret(V,P): V -> P, no-op forward. Tests forward and backward."""
        x = self._generate_inputs((4,), "tp", V)
        original = {r: x._local_tensors[r].clone() for r in range(self.WORLD_SIZE)}

        result = reinterpret(x, "tp", src=V, dst=P)

        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], original[r])
        self.assertIs(get_axis_local_type(result, "tp"), P)

        # Backward check
        grad_out = self._generate_inputs((4,), "tp", R)
        self._check_backward_not_none(_VaryingToPartial, (x, "tp"), grad_out)

    def test_reinterpret_same_type_noop(self):
        """reinterpret with same src and dst is identity."""
        x = self._generate_inputs((4,), "tp", R)
        result = reinterpret(x, "tp", src=R, dst=R)
        # Should return same tensor
        self.assertIs(result, x)
        self.assertIs(get_axis_local_type(result, "tp"), R)

    def test_reinterpret_shard_rejected(self):
        """reinterpret rejects Shard types."""
        x = self._generate_inputs((4,), "tp", R)
        with self.assertRaises(ValueError) as ctx:
            reinterpret(x, "tp", src=R, dst=S(0))
        self.assertIn("does not support S(i)", str(ctx.exception))

    def test_reinterpret_unsupported_transition(self):
        """reinterpret rejects unsupported transitions."""
        x = self._generate_inputs((4,), "tp", P)
        with self.assertRaises(ValueError) as ctx:
            reinterpret(x, "tp", src=P, dst=R)
        self.assertIn("not supported", str(ctx.exception))


class TestConvert(LocalTensorTestCase):
    """Test convert operations (semantics-preserving type coercion)."""

    def test_convert_r_to_v(self):
        """convert(R,V): R -> V, slices to local portion. Tests forward and backward."""
        # Create replicated input shaped for stack semantics
        base = torch.arange(6, dtype=torch.float).reshape(self.WORLD_SIZE, 2)
        x = self.mode.rank_map(lambda r: base.clone())
        assert_local_type(x, {"tp": R})

        result = convert(x, "tp", src=R, dst=V, dim=0)

        # Each rank gets its chunk: rank 0 gets [0,1], rank 1 gets [2,3], rank 2 gets [4,5]
        for r in range(self.WORLD_SIZE):
            expected = base[r]
            torch.testing.assert_close(
                result._local_tensors[r], expected, msg=f"rank {r}"
            )
        self.assertIs(get_axis_local_type(result, "tp"), V)

        # Backward check
        grad_out = self._generate_inputs((2,), "tp", V)
        self._check_backward_not_none(
            _ConvertReplicateToVarying, (x, "tp", 0, True), grad_out
        )

    def test_convert_r_to_shard(self):
        """convert(R,S(i)): R -> S(i), slices to local portion."""
        base = torch.arange(6, dtype=torch.float)
        x = self.mode.rank_map(lambda r: base.clone())
        assert_local_type(x, {"tp": R})

        result = convert(x, "tp", src=R, dst=S(0), dim=0)

        for r in range(self.WORLD_SIZE):
            expected = base[r * 2 : (r + 1) * 2]
            torch.testing.assert_close(
                result._local_tensors[r], expected, msg=f"rank {r}"
            )
        self.assertIs(get_axis_local_type(result, "tp"), V)

    def test_convert_i_to_v(self):
        """convert(I,V): I -> V, slices to local portion. Tests forward and backward."""
        base = torch.arange(6, dtype=torch.float).reshape(self.WORLD_SIZE, 2)
        x = self.mode.rank_map(lambda r: base.clone())
        assert_local_type(x, {"tp": I})

        result = convert(x, "tp", src=I, dst=V, dim=0)

        for r in range(self.WORLD_SIZE):
            expected = base[r]
            torch.testing.assert_close(
                result._local_tensors[r], expected, msg=f"rank {r}"
            )
        self.assertIs(get_axis_local_type(result, "tp"), V)

        # Backward check
        grad_out = self._generate_inputs((2,), "tp", V)
        self._check_backward_not_none(
            _ConvertInvariantToVarying, (x, "tp", 0, True), grad_out
        )

    def test_convert_i_to_shard(self):
        """convert(I,S(i)): I -> S(i), slices to local portion."""
        base = torch.arange(6, dtype=torch.float)
        x = self.mode.rank_map(lambda r: base.clone())
        assert_local_type(x, {"tp": I})

        result = convert(x, "tp", src=I, dst=S(0), dim=0)

        for r in range(self.WORLD_SIZE):
            expected = base[r * 2 : (r + 1) * 2]
            torch.testing.assert_close(
                result._local_tensors[r], expected, msg=f"rank {r}"
            )
        self.assertIs(get_axis_local_type(result, "tp"), V)

    def test_convert_r_to_p(self):
        """convert(R,P): R -> P, zeros out non-rank-0. Tests forward and backward."""
        base = torch.tensor([1.0, 2.0, 3.0])
        x = self.mode.rank_map(lambda r: base.clone())
        assert_local_type(x, {"tp": R})

        result = convert(x, "tp", src=R, dst=P)

        # Rank 0 keeps values, others are zeroed
        torch.testing.assert_close(result._local_tensors[0], base)
        for r in range(1, self.WORLD_SIZE):
            torch.testing.assert_close(
                result._local_tensors[r],
                torch.zeros_like(base),
                msg=f"rank {r} should be zeros",
            )
        self.assertIs(get_axis_local_type(result, "tp"), P)

        # Backward check
        grad_out = self._generate_inputs((3,), "tp", R)
        self._check_backward_not_none(_ConvertReplicateToPartial, (x, "tp"), grad_out)

    def test_convert_i_to_p(self):
        """convert(I,P): I -> P, zeros out non-rank-0. Tests forward and backward."""
        base = torch.tensor([1.0, 2.0, 3.0])
        x = self.mode.rank_map(lambda r: base.clone())
        assert_local_type(x, {"tp": I})

        result = convert(x, "tp", src=I, dst=P)

        torch.testing.assert_close(result._local_tensors[0], base)
        for r in range(1, self.WORLD_SIZE):
            torch.testing.assert_close(
                result._local_tensors[r],
                torch.zeros_like(base),
                msg=f"rank {r} should be zeros",
            )
        self.assertIs(get_axis_local_type(result, "tp"), P)

        # Backward check
        grad_out = self._generate_inputs((3,), "tp", I)
        self._check_backward_not_none(_ConvertInvariantToPartial, (x, "tp"), grad_out)

    def test_convert_v_to_p(self):
        """convert(V,P): V -> P, places data in disjoint positions. Tests forward and backward."""
        # Each rank has [r]
        x = self.mode.rank_map(lambda r: torch.tensor(float(r)))
        assert_local_type(x, {"tp": V})

        result = convert(x, "tp", src=V, dst=P, dim=0)

        # Each rank has a tensor of size world_size with its value at its position
        for r in range(self.WORLD_SIZE):
            expected = torch.zeros(self.WORLD_SIZE)
            expected[r] = float(r)
            torch.testing.assert_close(
                result._local_tensors[r], expected, msg=f"rank {r}"
            )
        self.assertIs(get_axis_local_type(result, "tp"), P)

        # Backward check
        grad_out = self._generate_inputs((self.WORLD_SIZE,), "tp", R)
        self._check_backward_not_none(
            _ConvertVaryingToPartial, (x, "tp", 0, True), grad_out
        )

    def test_convert_shard_to_p(self):
        """convert(S(i),P): S(i) -> P, places data in disjoint positions."""
        x = self.mode.rank_map(lambda r: torch.tensor([float(r)]))
        assert_local_type(x, {"tp": V})

        result = convert(x, "tp", src=S(0), dst=P, dim=0)

        for r in range(self.WORLD_SIZE):
            expected = torch.zeros(self.WORLD_SIZE)
            expected[r] = float(r)
            torch.testing.assert_close(
                result._local_tensors[r], expected, msg=f"rank {r}"
            )
        self.assertIs(get_axis_local_type(result, "tp"), P)

    def test_convert_same_type_noop(self):
        """convert with same src and dst is identity."""
        x = self._generate_inputs((4,), "tp", R)
        result = convert(x, "tp", src=R, dst=R)
        self.assertIs(result, x)
        self.assertIs(get_axis_local_type(result, "tp"), R)

    def test_convert_r_to_i_same_as_reinterpret(self):
        """convert(R,I) should work like reinterpret(R,I)."""
        x = self._generate_inputs((4,), "tp", R)
        original = {r: x._local_tensors[r].clone() for r in range(self.WORLD_SIZE)}

        result = convert(x, "tp", src=R, dst=I)

        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], original[r])
        self.assertIs(get_axis_local_type(result, "tp"), I)

    def test_convert_i_to_r_same_as_reinterpret(self):
        """convert(I,R) should work like reinterpret(I,R)."""
        x = self._generate_inputs((4,), "tp", I)
        original = {r: x._local_tensors[r].clone() for r in range(self.WORLD_SIZE)}

        result = convert(x, "tp", src=I, dst=R)

        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], original[r])
        self.assertIs(get_axis_local_type(result, "tp"), R)

    def test_convert_from_partial_error(self):
        """convert cannot convert from partial."""
        x = self._generate_inputs((4,), "tp", P)
        with self.assertRaises(ValueError) as ctx:
            convert(x, "tp", src=P, dst=R)
        self.assertIn("not supported", str(ctx.exception))


class TestMeshSetup(unittest.TestCase):
    """Test mesh setup functions."""

    def test_set_and_get_mesh(self):
        """Test set_mesh and get_mesh."""
        original = get_mesh()

        fake_mesh = object()
        set_mesh(fake_mesh)
        self.assertIs(get_mesh(), fake_mesh)

        # Restore
        set_mesh(original)


class TestReinterpretCompositions(LocalTensorTestCase):
    """Test compositional reinterpret operations: I->V and I->P."""

    def test_reinterpret_i_to_v(self):
        """reinterpret(I,V): I -> R -> V composition, no-op forward."""
        x = self._generate_inputs((4,), "tp", I)
        original = {r: x._local_tensors[r].clone() for r in range(self.WORLD_SIZE)}

        result = reinterpret(x, "tp", src=I, dst=V)

        # Forward is no-op (composition of two no-ops)
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], original[r])
        self.assertIs(get_axis_local_type(result, "tp"), V)

    def test_reinterpret_i_to_p(self):
        """reinterpret(I,P): I -> R -> P composition, no-op forward."""
        x = self._generate_inputs((4,), "tp", I)
        original = {r: x._local_tensors[r].clone() for r in range(self.WORLD_SIZE)}

        result = reinterpret(x, "tp", src=I, dst=P)

        # Forward is no-op (composition of two no-ops)
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], original[r])
        self.assertIs(get_axis_local_type(result, "tp"), P)


class TestAllGatherMultiDim(LocalTensorTestCase):
    """Test all_gather with different gather dimensions."""

    def test_all_gather_dim_1(self):
        """all_gather with S(1) concatenates on dim 1."""
        # Each rank has shape (2, 1)
        x = self.mode.rank_map(lambda r: torch.tensor([[float(r)], [float(r + 10)]]))
        assert_local_type(x, {"tp": V})

        result = all_gather(x, "tp", src=S(1), dst=R)

        # Result should have shape (2, 3) on all ranks (cat along dim 1)
        expected = torch.tensor([[0.0, 1.0, 2.0], [10.0, 11.0, 12.0]])
        self._assert_all_ranks_equal(result)
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], expected)
        self.assertIs(get_axis_local_type(result, "tp"), R)

    def test_all_gather_2d_tensors(self):
        """all_gather with 2D varying tensors."""
        # Each rank has a 2x2 matrix
        x = self.mode.rank_map(lambda r: torch.full((2, 2), float(r)))
        assert_local_type(x, {"tp": V})

        result = all_gather(x, "tp", src=V, dst=R)

        # Result should be (3, 2, 2) - stack along dim 0
        expected = torch.stack(
            [torch.full((2, 2), float(r)) for r in range(self.WORLD_SIZE)], dim=0
        )
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], expected)
        self.assertIs(get_axis_local_type(result, "tp"), R)


class TestAllToAllMultiDim(LocalTensorTestCase):
    """Test all_to_all with different split/concat dimensions."""

    def test_all_to_all_2d_same_dims(self):
        """all_to_all with 2D tensors, split and concat on same dim."""
        # Each rank has shape (3, 2) - split on dim 0
        x = self.mode.rank_map(
            lambda r: torch.arange(6, dtype=torch.float).reshape(3, 2) + r * 10
        )
        assert_local_type(x, {"tp": V})

        result = all_to_all(x, "tp", split_dim=0, concat_dim=0)

        # After all_to_all:
        # Rank 0 gets [0th row from rank 0, 0th row from rank 1, 0th row from rank 2]
        # etc.
        for r in range(self.WORLD_SIZE):
            result_tensor = result._local_tensors[r]
            self.assertEqual(result_tensor.shape, (3, 2))


class TestEinsumSingleOperand(unittest.TestCase):
    """Test einsum with single operand (unary operations).

    TODO: These tests depend on LTensor which was removed. They need to be
    reimplemented once the type-checking layer is rebuilt.
    """

    pass


class TestTypeSingletons(unittest.TestCase):
    """Test that type objects are singletons."""

    def test_replicate_is_singleton(self):
        """R should be a singleton."""
        self.assertIs(Replicate(), R)
        self.assertIs(Replicate(), Replicate())

    def test_invariant_is_singleton(self):
        """I should be a singleton."""
        self.assertIs(Invariant(), I)
        self.assertIs(Invariant(), Invariant())

    def test_varying_is_singleton(self):
        """V should be a singleton."""
        self.assertIs(Varying(), V)
        self.assertIs(Varying(), Varying())

    def test_partial_is_singleton(self):
        """P should be a singleton."""
        self.assertIs(Partial(), P)
        self.assertIs(Partial(), Partial())

    def test_shard_equality(self):
        """Shard objects with same dim should be equal."""
        self.assertEqual(S(0), S(0))
        self.assertEqual(S(1), S(1))
        self.assertNotEqual(S(0), S(1))
        self.assertNotEqual(S(0), V)

    def test_type_repr(self):
        """Test string representation of types."""
        self.assertEqual(repr(R), "R")
        self.assertEqual(repr(I), "I")
        self.assertEqual(repr(V), "V")
        self.assertEqual(repr(P), "P")
        self.assertEqual(repr(S(0)), "S(0)")
        self.assertEqual(repr(S(1)), "S(1)")


class TestBackwardType(unittest.TestCase):
    """Test backward_type method for all types."""

    def test_replicate_backward_type(self):
        """Replicate backward type is Partial."""
        self.assertIs(R.backward_type(), P)

    def test_invariant_backward_type(self):
        """Invariant backward type is Invariant."""
        self.assertIs(I.backward_type(), I)

    def test_varying_backward_type(self):
        """Varying backward type is Varying."""
        self.assertIs(V.backward_type(), V)

    def test_partial_backward_type(self):
        """Partial backward type is Replicate."""
        self.assertIs(P.backward_type(), R)

    def test_shard_backward_type(self):
        """Shard backward type is the same Shard."""
        s = S(0)
        self.assertEqual(s.backward_type(), s)
        s2 = S(1)
        self.assertEqual(s2.backward_type(), s2)


class TestPartitionSpec(unittest.TestCase):
    """Test PartitionSpec for Global SPMD."""

    def test_empty_partition_spec(self):
        """Empty partition spec is fully replicated."""
        ps = PartitionSpec()
        self.assertEqual(len(ps), 0)
        self.assertEqual(ps.get_mesh_axes(), set())

    def test_single_axis_partition_spec(self):
        """Partition spec with single mesh axis."""
        ps = PartitionSpec("tp", None)
        self.assertEqual(len(ps), 2)
        self.assertEqual(ps.get_mesh_axes(), {"tp"})
        self.assertEqual(ps[0], "tp")
        self.assertIsNone(ps[1])

    def test_multi_axis_partition_spec(self):
        """Partition spec with multiple mesh axes on same dim."""
        ps = PartitionSpec(("dp", "tp"), None)
        self.assertEqual(len(ps), 2)
        self.assertEqual(ps.get_mesh_axes(), {"dp", "tp"})
        self.assertEqual(ps[0], ("dp", "tp"))

    def test_partition_spec_iteration(self):
        """Partition spec should be iterable."""
        ps = PartitionSpec("tp", "dp", None)
        axes = list(ps)
        self.assertEqual(axes, ["tp", "dp", None])

    def test_partition_spec_repr(self):
        """Test string representation of PartitionSpec."""
        ps = PartitionSpec()
        self.assertEqual(repr(ps), "PartitionSpec()")

        ps = PartitionSpec("tp", None)
        self.assertEqual(repr(ps), "PartitionSpec('tp', None)")

        ps = PartitionSpec(("dp", "tp"), "ep")
        self.assertEqual(repr(ps), "PartitionSpec(('dp', 'tp'), 'ep')")


class TestRedistribute(LocalTensorTestCase):
    """Test redistribute operation (semantics-preserving with comms)."""

    def test_redistribute_v_to_r(self):
        """redistribute(V,R) uses all_gather."""
        x = self.mode.rank_map(lambda r: torch.tensor(float(r)))
        assert_local_type(x, {"tp": V})

        result = redistribute(x, "tp", src=V, dst=R, dim=0)

        expected = torch.tensor([0.0, 1.0, 2.0])
        self._assert_all_ranks_equal(result)
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], expected)
        self.assertIs(get_axis_local_type(result, "tp"), R)

    def test_redistribute_v_to_i(self):
        """redistribute(V,I) uses all_gather."""
        x = self.mode.rank_map(lambda r: torch.tensor(float(r)))
        assert_local_type(x, {"tp": V})

        result = redistribute(x, "tp", src=V, dst=I, dim=0)

        expected = torch.tensor([0.0, 1.0, 2.0])
        self._assert_all_ranks_equal(result)
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], expected)
        self.assertIs(get_axis_local_type(result, "tp"), I)

    def test_redistribute_p_to_r(self):
        """redistribute(P,R) uses all_reduce."""
        x = self._generate_inputs((4,), "tp", P)
        expected_sum = sum(x._local_tensors[r].clone() for r in range(self.WORLD_SIZE))

        result = redistribute(x, "tp", src=P, dst=R)

        self._assert_all_ranks_equal(result)
        for r in range(self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], expected_sum)
        self.assertIs(get_axis_local_type(result, "tp"), R)

    def test_redistribute_p_to_s(self):
        """redistribute(P,S(0)) uses reduce_scatter."""
        chunk_size = 2
        x = self.mode.rank_map(
            lambda r: torch.arange(self.WORLD_SIZE * chunk_size, dtype=torch.float) + r
        )
        assert_local_type(x, {"tp": P})

        result = redistribute(x, "tp", src=P, dst=S(0))

        for r in range(self.WORLD_SIZE):
            expected = torch.zeros(chunk_size)
            for src_rank in range(self.WORLD_SIZE):
                chunk_start = r * chunk_size
                chunk_end = (r + 1) * chunk_size
                expected += x._local_tensors[src_rank][chunk_start:chunk_end]
            torch.testing.assert_close(
                result._local_tensors[r], expected, msg=f"rank {r}"
            )
        self.assertIs(get_axis_local_type(result, "tp"), V)

    def test_redistribute_r_to_v_uses_convert(self):
        """redistribute(R,V) delegates to convert."""
        base = torch.arange(6, dtype=torch.float).reshape(self.WORLD_SIZE, 2)
        x = self.mode.rank_map(lambda r: base.clone())
        assert_local_type(x, {"tp": R})

        result = redistribute(x, "tp", src=R, dst=V, dim=0)

        for r in range(self.WORLD_SIZE):
            expected = base[r]
            torch.testing.assert_close(
                result._local_tensors[r], expected, msg=f"rank {r}"
            )
        self.assertIs(get_axis_local_type(result, "tp"), V)

    def test_redistribute_r_to_p_uses_convert(self):
        """redistribute(R,P) delegates to convert."""
        base = torch.tensor([1.0, 2.0, 3.0])
        x = self.mode.rank_map(lambda r: base.clone())
        assert_local_type(x, {"tp": R})

        result = redistribute(x, "tp", src=R, dst=P)

        torch.testing.assert_close(result._local_tensors[0], base)
        for r in range(1, self.WORLD_SIZE):
            torch.testing.assert_close(result._local_tensors[r], torch.zeros_like(base))
        self.assertIs(get_axis_local_type(result, "tp"), P)

    def test_redistribute_same_type_noop(self):
        """redistribute with same src and dst is identity."""
        x = self._generate_inputs((4,), "tp", R)
        result = redistribute(x, "tp", src=R, dst=R)
        self.assertIs(result, x)
        self.assertIs(get_axis_local_type(result, "tp"), R)

    def test_redistribute_shard_to_shard_uses_all_to_all(self):
        """redistribute(S(i),S(j)) with different dims uses all_to_all."""
        x = self.mode.rank_map(
            lambda r: torch.arange(6, dtype=torch.float).reshape(3, 2) + r * 10
        )
        assert_local_type(x, {"tp": V})

        result = redistribute(x, "tp", src=S(0), dst=S(1), dim=0)

        # Check shapes are correct
        for r in range(self.WORLD_SIZE):
            self.assertEqual(result._local_tensors[r].shape[0], 1)
            self.assertEqual(result._local_tensors[r].shape[1], 6)
        self.assertIs(get_axis_local_type(result, "tp"), V)


class TestLinearTypePropagation(LocalTensorTestCase):
    """Test type propagation through F.linear (matmul + optional bias)."""

    def test_linear_r_v_gives_v(self):
        """F.linear with R weight and V input should give V output, not R.

        Regression: V is canonicalized to {} so its type was silently dropped
        during per-axis type collection, making the output R instead of V.
        """
        inp = self._generate_inputs((2, 4), "tp", V)
        weight = self._generate_inputs((3, 4), "tp", R)
        result = torch.nn.functional.linear(inp, weight)
        self.assertIs(get_axis_local_type(result, "tp"), V)

    def test_matmul_r_v_gives_v(self):
        """torch.matmul with R and V should give V output."""
        x = self._generate_inputs((2, 4), "tp", R)
        y = self._generate_inputs((4, 3), "tp", V)
        result = torch.matmul(x, y)
        self.assertIs(get_axis_local_type(result, "tp"), V)

    def test_add_r_v_gives_v(self):
        """torch.add with R and V should give V output."""
        x = self._generate_inputs((4,), "tp", R)
        y = self._generate_inputs((4,), "tp", V)
        result = torch.add(x, y)
        self.assertIs(get_axis_local_type(result, "tp"), V)


class TestStrictMode(LocalTensorTestCase):
    """Test SpmdTypeMode strict mode which errors on unannotated tensors."""

    def setUp(self):
        """Enter LocalTensorMode and strict SpmdTypeMode for each test."""
        self.mode = LocalTensorMode(self.WORLD_SIZE)
        self.mode.__enter__()
        self.spmd_mode = SpmdTypeMode(strict=True)
        self.spmd_mode.__enter__()

    def test_strict_mixed_annotated_unannotated_fails(self):
        """Strict mode raises when one operand is annotated and the other is not."""
        x = self._generate_inputs((4,), "tp", R)
        y = self.mode.rank_map(lambda r: torch.randn(4))
        with self.assertRaises(SpmdTypeError):
            torch.add(x, y)

    def test_strict_mixed_cat_fails(self):
        """Strict mode catches mixed typed/untyped tensors inside lists (e.g. torch.cat)."""
        x = self._generate_inputs((4,), "tp", R)
        y = self.mode.rank_map(lambda r: torch.randn(4))
        with self.assertRaises(SpmdTypeError):
            torch.cat([x, y])

    def test_strict_all_annotated_passes(self):
        """Strict mode allows operations when all tensors are annotated."""
        x = self._generate_inputs((4,), "tp", R)
        y = self._generate_inputs((4,), "tp", R)
        result = torch.add(x, y)
        self.assertIs(get_axis_local_type(result, "tp"), R)

    def test_strict_all_v_annotated_mixed_with_unannotated_fails(self):
        """Strict mode catches all-V tensor (canonicalizes to {}) mixed with unannotated."""
        x = self._generate_inputs((4,), "tp", V)
        # x has _spmd_types attr (set to {} after canonicalization)
        y = self.mode.rank_map(lambda r: torch.randn(4))
        # y has no _spmd_types attr at all
        with self.assertRaises(SpmdTypeError):
            torch.add(x, y)

    def test_strict_all_unannotated_passes(self):
        """Strict mode allows operations when no tensors are annotated (no SPMD tracking)."""
        x = self.mode.rank_map(lambda r: torch.randn(4))
        y = self.mode.rank_map(lambda r: torch.randn(4))
        result = torch.add(x, y)  # Should not raise â€” no typed tensors
        self.assertEqual(get_axis_local_type(result, "tp"), V)

    def test_nonstrict_mixed_passes(self):
        """Non-strict mode (default) allows mixing annotated and unannotated tensors."""
        self.spmd_mode.__exit__(None, None, None)
        nonstrict = SpmdTypeMode(strict=False)
        nonstrict.__enter__()
        try:
            x = self._generate_inputs((4,), "tp", R)
            y = self.mode.rank_map(lambda r: torch.randn(4))
            result = torch.add(x, y)  # Should not raise
            self.assertIs(get_axis_local_type(result, "tp"), R)
        finally:
            nonstrict.__exit__(None, None, None)
            self.spmd_mode.__enter__()


class TestTypeErrorMessages(expecttest.TestCase):
    """Test that type errors include actionable fix suggestions.

    Tests call infer_output_type_for_axis / _infer_mul_output_type_for_axis
    directly rather than going through torch ops, because V (Varying) is
    canonicalized away in the type tracker and cannot appear in collected types.
    """

    def test_general_I_R(self):
        """I + R: suggest reinterpret I->R (I->V filtered: changes output from R to V)."""
        with self.assertRaises(SpmdTypeError) as ctx:
            infer_output_type_for_axis("tp", [I, R])
        self.assertExpectedInline(
            str(ctx.exception),
            """\
Invariant type on axis 'tp' cannot mix with other types. Found types: [I, R]
Are you missing a collective? e.g.,
  reinterpret(tensor, 'tp', src=I, dst=R) on the Invariant operand (no-op forward, all-reduce in backward)""",
        )

    def test_general_I_V(self):
        """I + V: suggest reinterpret I->R or I->V."""
        with self.assertRaises(SpmdTypeError) as ctx:
            infer_output_type_for_axis("tp", [I, V])
        self.assertExpectedInline(
            str(ctx.exception),
            """\
Invariant type on axis 'tp' cannot mix with other types. Found types: [I, V]
Are you missing a collective? e.g.,
  reinterpret(tensor, 'tp', src=I, dst=R) on the Invariant operand (no-op forward, all-reduce in backward)
  reinterpret(tensor, 'tp', src=I, dst=V) on the Invariant operand (no-op forward, all-reduce in backward)""",
        )

    def test_general_I_P(self):
        """I + P: suggest convert I->P or all_reduce P->I."""
        with self.assertRaises(SpmdTypeError) as ctx:
            infer_output_type_for_axis("tp", [I, P])
        self.assertExpectedInline(
            str(ctx.exception),
            """\
Invariant type on axis 'tp' cannot mix with other types. Found types: [I, P]
Are you missing a collective? e.g.,
  convert(tensor, 'tp', src=I, dst=P) on the Invariant operand (zeros non-rank-0 in forward, no-op backward)
  all_reduce(tensor, 'tp', src=P, dst=I) on the Partial operand (all-reduce in forward, no-op backward)""",
        )

    def test_general_P_R(self):
        """P + R: suggest all_reduce P->R or convert R->P."""
        with self.assertRaises(SpmdTypeError) as ctx:
            infer_output_type_for_axis("tp", [P, R])
        self.assertExpectedInline(
            str(ctx.exception),
            """\
Partial type on axis 'tp' can only combine with partial. Found types: [P, R]
Are you missing a collective? e.g.,
  all_reduce(tensor, 'tp', src=P, dst=R) on the Partial operand (all-reduce in forward, all-reduce in backward)
  convert(tensor, 'tp', src=R, dst=P) on the Replicate operand (zeros non-rank-0 in forward, zeros non-rank-0 in backward)""",
        )

    def test_general_P_V(self):
        """P + V: suggest all_reduce P->R."""
        with self.assertRaises(SpmdTypeError) as ctx:
            infer_output_type_for_axis("tp", [P, V])
        self.assertExpectedInline(
            str(ctx.exception),
            """\
Partial type on axis 'tp' can only combine with partial. Found types: [P, V]
Are you missing a collective? e.g.,
  all_reduce(tensor, 'tp', src=P, dst=R) on the Partial operand (all-reduce in forward, all-reduce in backward)""",
        )

    def test_general_R_P_V(self):
        """R + P + V: suggest all_reduce P->R."""
        with self.assertRaises(SpmdTypeError) as ctx:
            infer_output_type_for_axis("tp", [R, P, V])
        self.assertExpectedInline(
            str(ctx.exception),
            """\
Partial type on axis 'tp' can only combine with partial. Found types: [R, P, V]
Are you missing a collective? e.g.,
  all_reduce(tensor, 'tp', src=P, dst=R) on the Partial operand (all-reduce in forward, all-reduce in backward)""",
        )

    def test_general_I_R_V(self):
        """I + R + V: suggest reinterpret I->R or I->V."""
        with self.assertRaises(SpmdTypeError) as ctx:
            infer_output_type_for_axis("tp", [I, R, V])
        self.assertExpectedInline(
            str(ctx.exception),
            """\
Invariant type on axis 'tp' cannot mix with other types. Found types: [I, R, V]
Are you missing a collective? e.g.,
  reinterpret(tensor, 'tp', src=I, dst=R) on the Invariant operand (no-op forward, all-reduce in backward)
  reinterpret(tensor, 'tp', src=I, dst=V) on the Invariant operand (no-op forward, all-reduce in backward)""",
        )

    def test_mul_P_P(self):
        """P * P: suggest all_reduce P->R."""
        with self.assertRaises(SpmdTypeError) as ctx:
            _infer_mul_output_type_for_axis("tp", [P, P])
        self.assertExpectedInline(
            str(ctx.exception),
            """\
Partial * Partial on axis 'tp' is forbidden (not linear). Found types: [P, P]
Are you missing a collective? e.g.,
  all_reduce(tensor, 'tp', src=P, dst=R) on the Partial operand (all-reduce in forward, all-reduce in backward)""",
        )

    def test_mul_P_V(self):
        """P * V: suggest all_reduce P->R."""
        with self.assertRaises(SpmdTypeError) as ctx:
            _infer_mul_output_type_for_axis("tp", [P, V])
        self.assertExpectedInline(
            str(ctx.exception),
            """\
Partial type on axis 'tp' can only multiply with Replicate. Found types: [P, V]
Are you missing a collective? e.g.,
  all_reduce(tensor, 'tp', src=P, dst=R) on the Partial operand (all-reduce in forward, all-reduce in backward)""",
        )

    def test_mul_P_I(self):
        """P * I: suggest reinterpret I->R or all_reduce P->I."""
        with self.assertRaises(SpmdTypeError) as ctx:
            _infer_mul_output_type_for_axis("tp", [P, I])
        self.assertExpectedInline(
            str(ctx.exception),
            """\
Partial type on axis 'tp' can only multiply with Replicate. Found types: [P, I]
Are you missing a collective? e.g.,
  reinterpret(tensor, 'tp', src=I, dst=R) on the Invariant operand (no-op forward, all-reduce in backward)
  all_reduce(tensor, 'tp', src=P, dst=I) on the Partial operand (all-reduce in forward, no-op backward)""",
        )


class TestRejectsShard(unittest.TestCase):
    """Test that type-setting APIs reject Shard types (only local SPMD types allowed)."""

    def test_assert_local_type_rejects_shard(self):
        """assert_local_type should reject S(i) since it's not a PerMeshAxisLocalSpmdType."""
        x = torch.randn(4)
        with self.assertRaises(TypeError):
            assert_local_type(x, {"tp": S(0)})


if __name__ == "__main__":
    unittest.main()
